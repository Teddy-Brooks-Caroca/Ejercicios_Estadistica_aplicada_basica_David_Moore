
La **regresión lineal simple** busca modelar cómo cambia una **variable respuesta** (`(y)`) en función de una **variable explicativa** (`(x)`). El modelo más común es la **recta de mínimos cuadrados**, cuya ecuación es:

$$  
\hat{y} = a + b x  
$$

donde:

- **(`a`)** es la ordenada al origen (intercepto). Representa el valor esperado de (`y`) cuando (`x = 0`). Puede no tener sentido práctico si (`x=0`) no está dentro del rango de datos.
    
- **(`b`)** es la pendiente. Indica cuánto cambia (`y`) (en promedio) cuando (`x`) aumenta en una unidad.


La recta se calcula de forma que minimice la **suma de cuadrados de los residuos**:

$$  
\sum (y_i - \hat{y}_i)^2  
$$

---

## Cálculo en R

La función principal es **`lm()`** (_linear model_).

```r
set.seed(123)
x <- 1:50
y <- 3 + 2 * x + rnorm(50, 0, 10)

modelo <- lm(y ~ x)
summary(modelo)
```

Salida parcial:

- **Estimate (Intercept)** → valor de (`a`).
    
- **Estimate (`x`)** → valor de (`b`).
    

La ecuación ajustada es, por ejemplo:  
$$  
\hat{y} = 3.5 + 1.98x  
$$

---

## Interpretación

- **Pendiente (`b`):** mide el efecto medio de (`x`) sobre (`y`).
    
- **Intercepto (`a`):** valor de (`y`) cuando (`x=0`). Solo interpretable si (`x`) puede tomar valores cercanos a 0.
    
- **Predicción:** podemos predecir (`y`) dado un nuevo valor de (`x`):
    

```r
predict(modelo, newdata = data.frame(x = 25))
```

---

## Relación con la correlación

La regresión y la correlación están estrechamente ligadas:

- Si estandarizamos (`x`) e (`y`), la pendiente de la recta es la **correlación (`r`)**.
    
- El **coeficiente de determinación (`R^2` = `r^2`)** mide la proporción de la variabilidad de (`y`) explicada por la recta.

Ejemplo en R:

```r
summary(modelo)$r.squared
```

Esto devuelve (`R^2`). Un valor cercano a 1 indica que la recta explica gran parte de la variabilidad.

---

## Evaluación del ajuste

Es crucial examinar los **residuos** (diferencias entre valores observados y predichos).

```r
plot(modelo$residuals)
abline(h = 0, col = "red")
```

Aspectos clave:

- **Patrones no aleatorios** en los residuos sugieren que una recta no es adecuada.
    
- **Outliers:** puntos con residuos inusualmente grandes.
    
- **Observaciones influyentes:** valores extremos en (`x`) que pueden modificar de forma drástica la pendiente o intercepto.

---

### En síntesis

- La **recta de regresión** resume la relación lineal entre dos variables.
    
- En R se ajusta con `lm()`.
    
- La pendiente indica el efecto promedio de (`x`) sobre (`y`).
    
- La correlación y (`R^2`) están directamente relacionados.
    
- El análisis de **residuos, outliers e influencias** es fundamental para validar el modelo.

---
